<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Machine Learning nano-degree notebook (Unsupervised Learning)</title>
  <meta name="description" content="Udacity machine learning nano-degree notebook.">

  <!-- CSS files -->
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/tomorrow-night-bright.css">


  <link rel="canonical" href="/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2">
  <link rel="alternate" type="application/rss+xml" title="Neverland" href=" /feed.xml " />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="/favicon.png">
</head>


<body>
  
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <img src="/img/avatar.jpg" alt="" class="avatar">
  
  <a href="/" class="author_name">ZHENG ZHOU</a>
  <span class="author_job">Student (Master of Science)</span>
  <span class="author_bio mbm">I am undating myself...</span>
  <nav class="nav">
    <ul class="nav-list">
         
      <li class="nav-item">
        <a href="/archive/">Archive</a>
        <span>/</span>
      </li>
          
      <li class="nav-item">
        <a href="/categories/">Categories</a>
        <span>/</span>
      </li>
            
      <li class="nav-item">
        <a href="/tags/">Tags</a>
      </li>
       
    </ul>
  </nav>
  <div class="social-links">
  <ul>
    <li><a href="mailto:encorechow1992@gmail.com" class="social-link-item" target="_blank"><i class="fa fa-fw fa-envelope"></i></a></li>
    
    <li><a href="http://facebook.com/axieandyangyang" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="http://linkedin.com/in/zhou-zheng-378041a5" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    <li><a href="http://github.com/encorechow" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

  <nav class="nav">
  <ul class="nav-list">
    <li class="nav-item">
      <a href="/aboutme/">About Me</a>
    </li>
  </ul>
</nav>

</div>

      </div>
    </div>

    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "/" >
  Home
</a>

<div class="post-image-feature">
  <img class="feature-image" src=
  
  "/img/ml-pic.jpg"
  
  alt="Machine Learning nano-degree notebook (Unsupervised Learning) feature image">

  
</div><!-- /.image-wrap -->


<div id="post">
  <header class="post-header">
    <h1 title="Machine Learning nano-degree notebook (Unsupervised Learning)">Machine Learning nano-degree notebook (Unsupervised Learning)</h1>
    <span class="post-meta">
      <span class="post-date">
        21 NOV 2016
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    9 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h1 id="unsupervised-learning">Unsupervised Learning</h1>

<h2 id="clustering">1. Clustering</h2>

<p><a href="http://scikit-learn.org/stable/modules/clustering.html">Sklearn Clustering Algorithm Interface</a></p>

<h3 id="single-linkage-clustering">1.1. Single Linkage Clustering</h3>
<p>The simplest clustering which has following features:</p>

<ul>
  <li>consider each object a cluster (n objects).</li>
  <li>define inter-cluster distance as the distance between the closest two points in the two clusters. (Can be average or farthest two points, they have different name)</li>
  <li>merge two closest clusters</li>
  <li>repeat <script type="math/tex">n-k</script> times to make k clusters</li>
</ul>

<p>This gives us a hierarchical agglomerative tree structure.</p>

<h3 id="soft-clustering">1.2. Soft Clustering</h3>

<p>Instead of finding out which data point belongs to which cluster like k-mean algorithm, in soft clustering, we will try to find out what the probability of a specific data point belongs to some hypothesis (which is the mean of some gaussian).</p>

<p>Assume the data was generate by:</p>

<ol>
  <li>Select one of K gaussians (with fixed k means and variance) uniformly (So the prior can be ignored)</li>
  <li>Sample <script type="math/tex">X_i</script> from that gaussian</li>
  <li>Repeat n times</li>
</ol>

<p>Task: Find a hypothesis <script type="math/tex">% <![CDATA[
h = <\mu_1,...,\mu_k> %]]></script> that maximize the probability of the data (Maximum likelihood)</p>

<p>P.S. Hidden variable is the variables that are inferred from other observed variables</p>

<h3 id="expectation-maximization">1.2. Expectation Maximization</h3>

<p><strong>Expectation (define Z from <script type="math/tex">\mu</script> and it is soft clustering, analogy of assigning data to the cluster in K-mean algorithm):</strong></p>

<script type="math/tex; mode=display">\mathbf{E}[Z_{i,j}] = \frac{P(x=x_i|\mu=\mu_j)}{\sum_{i=1}^{k} P(x=x_i|\mu=\mu_j)}</script>

<p><script type="math/tex">Z_{i,j}</script> stands for the probability of observed <script type="math/tex">x_i</script> produced by the gaussian with <script type="math/tex">\mu_j</script></p>

<p>Additionally:</p>

<script type="math/tex; mode=display">P(x=x_i|\mu=\mu_j) = e^{-\frac{1}{2}\sigma^2(x_i-\mu_i)^2}</script>

<p><strong>Maximization (define <script type="math/tex">\mu</script> from Z, analogy of computing mean each iteration in K-mean algorithm)</strong></p>

<script type="math/tex; mode=display">\mu_{j} = \frac{\sum_i \mathbf{E}[Z_{i,j}]x_i}{\sum_{i} \mathbf{E}[Z_{i,j}]}</script>

<p>This can be transformed to K-mean algorithm if cluster assignments use argmax (which ends up with only 0 and 1, 0 stands for not belonging to that class and 1 otherwise).</p>

<p><strong>Properties of EM:</strong></p>

<ul>
  <li>monotonically non-decreasing likelihood</li>
  <li>does not converge (practically does)</li>
  <li>will not diverge</li>
  <li>can get stuck (can randomly restart)</li>
  <li>works with any distribution (if E (Bayes net stuff), M (Counting things) solvable)</li>
</ul>

<h3 id="clustering-properties">1.3. Clustering Properties</h3>

<ul>
  <li>Richness<br />
For any assignment of objects to clusters, there is some distance matrix D such that <script type="math/tex">P_D</script> return that clustering <script type="math/tex">\forall \space C \space \exists D P_D=C</script></li>
  <li>
    <p>Scale-invariance<br />
Scaling distance by a positive value does not change the clustering <script type="math/tex">\forall D \space \forall K>0 P_D = P_{KD}</script></p>
  </li>
  <li>Consistency<br />
Shrinking intra-cluster distances and expanding inter-cluster distances does not change the clustering <script type="math/tex">P_D = P_{D^{'}}</script></li>
</ul>

<p>So what consistency says is if you found that a bunch of things were similar, and a bunch of other things were dissimilar, that if you made the things that were similar more similar, and the things that were not similar less similar, it shouldn’t change your notion which things are similar and which things are not.</p>

<h3 id="impossibility-theorem">1.4. Impossibility Theorem</h3>

<p>No clustering schema can achieve all above three properties. These three properties are mutually contradiction in a sense. (Proven by Kleinberg)</p>

<h3 id="summary">Summary</h3>

<ul>
  <li>Clustering</li>
  <li>Connection to compact description</li>
  <li>Algorithm
    <ul>
      <li>K-means</li>
      <li>SLC (terminates fast)</li>
      <li>EM (soft clusters)</li>
    </ul>
  </li>
  <li>Clustering properties &amp; impossibility</li>
</ul>

<p><em>————————————————————————————— Updating… Nov. 30, 2016</em></p>

<h2 id="feature-engineering">2. Feature Engineering</h2>

<h3 id="min-max-feature-scaling">2.1. Min-Max Feature Scaling</h3>

<p>Unbalanced features will cause problem, for example, height and weight of an person. The numeric meaning of height and weight is quite off and should never be operated by using plus or somethings. That’s why we need feature scaling to make them somehow in a balanced space. (Ususally between 0 and 1)</p>

<p>Typically, the formula for feature scaling is like:</p>

<script type="math/tex; mode=display">x^{'}=\frac{x-x_{min}}{x_{max}-x_{min}}</script>

<p>But outlier will mess up the rescaling if use this formula.</p>

<h3 id="feature-selection">2.2. Feature Selection</h3>

<ul>
  <li>Knowledge Discovery
    <ul>
      <li>Interpretability</li>
      <li>Insight</li>
    </ul>
  </li>
  <li>Curse of Dimensionality</li>
</ul>

<p>Feature Selection can be exponentially hard since their are exponential number of feature subset for a given number of features.</p>

<p>Two potential algorithms that do feature selection: Filtering and Wrapping</p>

<h4 id="filtering">2.2.1. Filtering</h4>

<p>Image feature searching is a black box, we input our features into this black box, and it will output the subset of features that this black box thinks are most important.</p>

<p><img src="\assets\mlnd\filtering.png" alt="" /></p>

<p>The search box can be any feature selection criteria. For example, a Decision Tree, the criteria will be information gain.</p>

<h4 id="wrapping">2.2.2. Wrapping</h4>

<p>In contrast with filtering, wrapping is trying to select a subset of features and train the model inside the box.</p>

<p><img src="\assets\mlnd\wrapping.png" alt="" /></p>

<p>It extremely time consuming. But still, there are a couple of way to avoid such huge time consumption:<br />
- Forward<br />
  - try a single feature among all features at first, evaluate and choose the best<br />
  - pick up more features and repeat until the evaluation result has no significant changes.</p>

<ul>
  <li>Backward
    <ul>
      <li>try full set of features at first and evaluate.</li>
      <li>gradually reduce features one by one and evaluate. Stop when there is no significant changes.</li>
    </ul>
  </li>
  <li>Randomized Optimization
    <ul>
      <li>try the randomized opt algorithms that are available.</li>
    </ul>
  </li>
</ul>

<h3 id="relevance">2.3. Relevance</h3>

<ul>
  <li><script type="math/tex">x_i</script> is strongly relevant if removing it degrades the Bayes optimal classifier (B.O.C).</li>
  <li><script type="math/tex">x_i</script> is weakly relevant if:
    <ul>
      <li>not strongly relevant</li>
      <li><script type="math/tex">\exists</script> subset of features S such that adding <script type="math/tex">x_i</script> to S improves B.O.C.</li>
    </ul>
  </li>
  <li><script type="math/tex">x_i</script> is otherwise irrelevant</li>
</ul>

<p>Relevance is actually about information.</p>

<h3 id="relevance-vs-usefulness">2.4. Relevance vs Usefulness</h3>

<ul>
  <li>Relevance measures effect on B.O.C.</li>
  <li>Usefulness measures effect on a particular prediction.</li>
</ul>

<p>Usefulness is more about error instead of infomation/model/learner.</p>

<h3 id="summary-1">Summary</h3>

<ol>
  <li>Feature Selection</li>
  <li>Filtering (faster but ignore bias) vs Wrapping (slow but useful)</li>
  <li>Relevance (strong vs weak) vs Usefulness</li>
</ol>

<p><em>————————————————————————————— Updating… Dec. 2, 2016</em></p>

<h2 id="dimensionality-reduction">3. Dimensionality Reduction</h2>

<h3 id="feature-transformation">3.1. Feature Transformation</h3>

<p>Definition of <strong>Feature Transformation:</strong> As opposed to feature selection, feature transformation is about doing some kind of pre-processing on a set of features, in order to create a new set of features (smaller or compact). When creating these set of features, the information should be explicitly retained as much as possible.</p>

<script type="math/tex; mode=display">x -> F^N -> F^U</script>

<p>New feature set is just a linear combination of original feature by applying some transformation.</p>

<p>Feature selection is in fact a subset of feature transformation while feature selection just purely chooses a subset of the features.</p>

<p>Example problem: Information retrieval (like search on google)</p>

<ul>
  <li>A word has different meanings. (polysemy)</li>
  <li>Many words have similar meanings. (synonomy)</li>
</ul>

<h4 id="principle-component-analysis-pca">3.1.2. Principle Component Analysis (PCA)</h4>

<p>Basically when the problem has a large number of features, PCA will be applied to make a composite feature that more directly probes the underlying phenomenon of the problem. It also a very powerful standalone method in its own right for unsupervised learning.</p>

<p>PCA projects the data into a space that has lower dimensions.</p>

<p>Two definition of variance:</p>

<ul>
  <li>The willingness / flexibility of an algorithm to learn. (for model)</li>
  <li>technical term in statistics – roughly the “spread” of a data distribution (similar to standard deviation)</li>
</ul>

<p>What PCA do is trying to find a principle component that maximize the variance of data. By this way, the informations of original data are maximized.</p>

<p>Furthermore, PCA essentially derives the latent variables from original variables. Things like square footage and number of rooms of a house actually can be expressed just using the size of the house.</p>

<p><strong>Maximal Variance and Information Loss:</strong></p>

<p>Information Loss when mapping the original data onto principle component is actually the distance of projection of that data point. So in effect, the projection onto direction of maximal variance minimizes distance (information loss).</p>

<p>The goal of PCA is always minimizes the information loss from the space of high dimensions.</p>

<p><strong>When to use PCA</strong></p>

<ul>
  <li>Latent feature driving the pattern in data.</li>
  <li>Dimensionality Reduction.
    <ul>
      <li>visualize high dimensional data</li>
      <li>reduce noise</li>
      <li>make other algorithms work better. (eigenfaces)</li>
    </ul>
  </li>
</ul>

<h4 id="independent-components-analysis-ica">3.1.3. Independent Components Analysis (ICA)</h4>

<ul>
  <li>PCA is about finding correlation, which maximize variance. ==&gt; reconstruction</li>
  <li>ICA is trying to maximize <strong>Independence</strong>. It tries to find a linear transformation of feature space into a new feature space such that each of the individual new features are mutually independent.</li>
</ul>

<p>What ICA finally achieve, is that mutual information of each pair of new features <script type="math/tex">I(y_i,y_j) = 0</script>, and mutual information of all original features and new features <script type="math/tex">I(Y, X)</script> is as high as possible.</p>

<p>One example: <a href="http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi">Cocktail Party Problem</a></p>

<p>In this example, all microphones are the observables. These observables combine a mix of a bunch of sound sources. The sound sources are exactly hidden variable. By given these observable microphones, ICA will separate the sound sources from them such that the mutual information of the sound sources is 0 and the mutual information between sound sources and microphones is as high as possible.</p>

<h3 id="pca-vs-ica">3.2. PCA vs ICA</h3>

<ul>
  <li>Mutual orthogonal (PCA)</li>
  <li>Mutual independence (ICA)</li>
  <li>Maximal variance (PCA, actually finding orthogonal gaussian since gaussian has high variance)</li>
  <li>Maximal mutual information (ICA)</li>
  <li>Ordered features (PCA)</li>
  <li>Bag of features (ICA)</li>
</ul>

<p>Examples of how PCA and ICA differ:</p>

<ol>
  <li><strong>Blind Source Separation Problem</strong>
    <ul>
      <li>ICA well separate the sources</li>
      <li>PCA does a terrible job on it since PCA just transform the mix of sources into another mix of sources.</li>
    </ul>
  </li>
  <li><strong>Directional</strong>
    <ul>
      <li>It doesn’t matter if the input features are a matrix or a transpose of matrix for PCA. It ends up with finding same answer.</li>
      <li>ICA has direction for the input features, highly directional.</li>
    </ul>
  </li>
  <li><strong>Face images</strong>
    <ul>
      <li>The first component of PCA for images will be brightness of the images. That’s not actually helpful. So typically people always normalize all of the images on brightness. The second component of PCA will be average faces on this problem (eigenfaces). It’s helpful for reconstructing. PCA is kind of doing global orthogonality things on original features so that it is forced to finding global features over all original features.</li>
      <li>The ICA is trying to find the nose, the eyes, the mouse, the hair in the images, which are mutual independent. It does not care about orthogonality.</li>
    </ul>
  </li>
  <li><strong>Natural Scenes</strong>
    <ul>
      <li>ICA finds edges in natural scenes.</li>
    </ul>
  </li>
  <li><strong>Documents</strong>
    <ul>
      <li>ICA gives topics in given documents.</li>
    </ul>
  </li>
</ol>

<p>Overall, ICA allows to do analysis over the data to discover fundamental features of them, like edges, topics, etc. PCA tends to find the global features over the data.</p>

<h3 id="alternatives">3.3. Alternatives</h3>

<ul>
  <li>RCA = Random Components Analysis (generates random directions. It manages to work! and cheaper and easier than PCA)</li>
  <li>LDA = Linear Discriminant Analysis (finds a projection that discriminates based on the label, more like supervised learning)</li>
</ul>

<p><em>————————————————————————————— Updating… Dec. 9, 2016</em></p>

  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=/articles/2016-11/Machine-Learning-Nanodegree-Notebook-2" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->



        <footer>
  &copy; 2016 ZHENG ZHOU. Powered by <a href="http://jekyllrb.com/">Jekyll</a>
</footer>

      </div>
    </div>

  </div>
  <script type="text/javascript" src="/js/jquery-2.1.4.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</script>


</body>
</html>
